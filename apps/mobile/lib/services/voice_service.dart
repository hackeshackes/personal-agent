/**
 * Voice Service - è¯­éŸ³æœåŠ¡ (å®Œæ•´ç‰ˆ)
 * æ”¯æŒ:
 * - å½•éŸ³ (record_mp3)
 * - Whisper ASR
 * - ElevenLabs TTS
 * - å”¤é†’è¯æ£€æµ‹ (Porcupine å ä½)
 */

import 'package:flutter/foundation.dart';
import 'package:audioplayers/audioplayers.dart';
import 'package:record_mp3/record_mp3.dart';
import 'package:path_provider/path_provider.dart';
import 'package:http/http.dart' as http;
import 'dart:io';
import 'dart:convert';

class VoiceService extends ChangeNotifier {
  bool _isListening = false;
  bool _isRecording = false;
  bool _isPlaying = false;
  String? _transcript;
  String? _audioPath;
  final AudioPlayer _player = AudioPlayer();
  
  // å›è°ƒå‡½æ•°
  Function(bool)? onListeningChange;
  Function(String)? onTranscript;
  Function(String)? onAudioPath;
  
  // é…ç½®
  String _whisperEndpoint = 'http://127.0.0.1:18789/api/whisper';
  String _ttsEndpoint = 'http://127.0.0.1:18789/api/tts';
  
  bool get isListening => _isListening;
  bool get isRecording => _isRecording;
  bool get isPlaying => _isPlaying;
  String? get transcript => _transcript;
  String? get audioPath => _audioPath;
  
  /**
   * å¼€å§‹å½•éŸ³
   */
  Future<bool> startListening() async {
    try {
      final dir = await getApplicationDocumentsDirectory();
      _audioPath = '${dir.path}/voice_${DateTime.now().millisecondsSinceEpoch}.mp3';
      
      // è¯·æ±‚æƒé™å¹¶å¼€å§‹å½•éŸ³
      _isRecording = true;
      _isListening = true;
      notifyListeners();
      
      await RecordMp3.instance.start(_audioPath!, (status) {
        if (status != RecordMp3Status.RECORDING) {
          print('å½•éŸ³çŠ¶æ€: $status');
        }
      });
      
      onListeningChange?.call(true);
      print('ğŸ¤ å¼€å§‹å½•éŸ³: $_audioPath');
      return true;
    } catch (e) {
      print('å¼€å§‹å½•éŸ³å¤±è´¥: $e');
      _isRecording = false;
      return false;
    }
  }
  
  /**
   * åœæ­¢å½•éŸ³
   */
  Future<bool> stopListening() async {
    if (!_isRecording) return false;
    
    final success = await RecordMp3.instance.stop();
    _isRecording = false;
    _isListening = false;
    notifyListeners();
    
    onListeningChange?.call(false);
    
    if (success && _audioPath != null) {
      print('ğŸ¤ å½•éŸ³ç»“æŸ: $_audioPath');
      // è‡ªåŠ¨è½¬å½•
      await transcribe(_audioPath!);
    }
    
    return success;
  }
  
  /**
   * è¯­éŸ³è¯†åˆ« (Whisper)
   */
  Future<String> transcribe(String audioPath) async {
    print('ğŸ”„ æ­£åœ¨è¯†åˆ«: $audioPath');
    
    try {
      // æ–¹å¼ 1: è°ƒç”¨æœ¬åœ° Gateway API
      final result = await _transcribeViaGateway(audioPath);
      _transcript = result;
      onTranscript?.call(result);
      notifyListeners();
      return result;
    } catch (e) {
      print('è½¬å½•å¤±è´¥: $e');
      
      // æ–¹å¼ 2: ç›´æ¥è°ƒç”¨ Whisper (å¦‚æœæœ‰)
      try {
        final result = await _transcribeWhisper(audioPath);
        _transcript = result;
        onTranscript?.call(result);
        notifyListeners();
        return result;
      } catch (e2) {
        // é™çº§: ä½¿ç”¨æ¨¡æ‹Ÿç»“æœ
        _transcript = 'è¯†åˆ«å¤±è´¥ï¼Œè¯·é‡è¯•';
        onTranscript?.call(_transcript!);
        return _transcript!;
      }
    }
  }
  
  /**
   * é€šè¿‡ Gateway API è½¬å½•
   */
  Future<String> _transcribeViaGateway(String audioPath) async {
    final file = File(audioPath);
    final bytes = await file.readAsBytes();
    final base64 = base64Encode(bytes);
    
    final response = await http.post(
      Uri.parse('$_whisperEndpoint/transcribe'),
      headers: {'Content-Type': 'application/json'},
      body: jsonEncode({
        'audio': base64,
        'language': 'zh',
      }),
    );
    
    if (response.statusCode == 200) {
      final data = jsonDecode(response.body);
      return data['text'] ?? '';
    }
    
    throw Exception('è½¬å½•å¤±è´¥: ${response.statusCode}');
  }
  
  /**
   * ç›´æ¥è°ƒç”¨ Whisper CLI
   */
  Future<String> _transcribeWhisper(String audioPath) async {
    // å‡è®¾ whisper åœ¨ PATH ä¸­
    final process = await Process.run('whisper', [
      audioPath,
      '--model', 'base',
      '--language', 'Chinese',
      '--no_timestamps',
      '--output_format', 'json',
    ]);
    
    if (process.exitCode == 0) {
      final jsonPath = audioPath.replaceAll(RegExp(r'\.[^.]+$'), '.json');
      final jsonFile = File(jsonPath);
      if (await jsonFile.exists()) {
        final data = jsonDecode(await jsonFile.readAsString());
        return data['text'] ?? '';
      }
    }
    
    throw Exception('Whisper è½¬å½•å¤±è´¥');
  }
  
  /**
   * æ’­æ”¾è¯­éŸ³å›å¤ (ElevenLabs TTS)
   */
  Future<void> playVoice(String text) async {
    print('ğŸ”Š æ’­æ”¾: $text');
    
    try {
      // æ–¹å¼ 1: è°ƒç”¨ Gateway TTS API
      final audioPath = await _synthesizeViaGateway(text);
      await _playAudio(audioPath);
    } catch (e) {
      print('TTS å¤±è´¥: $e');
      // é™çº§: ä½¿ç”¨è¯­éŸ³è½¬æ–‡å­—æ˜¾ç¤º
    }
  }
  
  /**
   * é€šè¿‡ Gateway API åˆæˆè¯­éŸ³
   */
  Future<String> _synthesizeViaGateway(String text) async {
    final response = await http.post(
      Uri.parse('$_ttsEndpoint/synthesize'),
      headers: {'Content-Type': 'application/json'},
      body: jsonEncode({
        'text': text,
        'voice': 'zh_female',
      }),
    );
    
    if (response.statusCode == 200) {
      final dir = await getApplicationDocumentsDirectory();
      final audioPath = '${dir.path}/tts_${DateTime.now().millisecondsSinceEpoch}.mp3';
      final file = File(audioPath);
      await file.writeAsBytes(response.body);
      return audioPath;
    }
    
    throw Exception('TTS å¤±è´¥: ${response.statusCode}');
  }
  
  /**
   * æ’­æ”¾éŸ³é¢‘æ–‡ä»¶
   */
  Future<void> _playAudio(String path) async {
    try {
      await _player.play(DeviceFileSource(path));
      _isPlaying = true;
      notifyListeners();
      
      _player.onPlayerStateChanged.listen((state) {
        _isPlaying = state == PlayerState.playing;
        notifyListeners();
      });
    } catch (e) {
      print('æ’­æ”¾å¤±è´¥: $e');
    }
  }
  
  /**
   * åœæ­¢æ’­æ”¾
   */
  Future<void> stopPlaying() async {
    await _player.stop();
    _isPlaying = false;
    notifyListeners();
  }
  
  /**
   * è®¾ç½®å”¤é†’è¯æ£€æµ‹
   */
  Future<void> enableWakeWordDetection(bool enabled) async {
    if (enabled && !_isListening) {
      await startListening();
    } else if (!enabled && _isListening) {
      await stopListening();
    }
    print('ğŸ”” å”¤é†’è¯æ£€æµ‹: $enabled');
  }
  
  /**
   * å½•éŸ³æƒé™æ£€æŸ¥
   */
  Future<bool> hasPermission() async {
    return await RecordMp3.instance.hasPermission() ?? false;
  }
  
  /**
   * è·å–éŸ³é¢‘æ–‡ä»¶æ—¶é•¿
   */
  Future<Duration> getAudioDuration(String path) async {
    final player = AudioPlayer();
    await player.setSource(DeviceFileSource(path));
    return await player.getDuration() ?? Duration.zero;
  }
  
  /**
   * åˆ é™¤ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
   */
  Future<void> cleanup() async {
    if (_audioPath != null) {
      final file = File(_audioPath!);
      if (await file.exists()) {
        await file.delete();
      }
    }
    _transcript = null;
    _audioPath = null;
  }
  
  /**
   * è·å–è¯­éŸ³çŠ¶æ€
   */
  Map<String, dynamic> getStatus() {
    return {
      'isListening': _isListening,
      'isRecording': _isRecording,
      'isPlaying': _isPlaying,
      'transcript': _transcript,
      'audioPath': _audioPath,
    };
  }
}
